---
title: 'Chapter 13: Classification'
output: html_document
---

```{r setup, include=FALSE}
# Thanks to Seaam Noor for some excellent work on this script.

# There are two packages which you need to install.

# install.packages("tidymodels")
# install.packages("rpart.plot")
# install.packages("randomForest")

knitr::opts_chunk$set(echo = TRUE)
library(broom)
library(infer)
library(skimr)
library(gganimate)
library(rpart.plot)
library(tidymodels)
library(tidyverse)

# tidymodels really wants your dependent variable to be a factor. After all, you
# have told tidymodels that it is not really a number --- it is a category with
# two possible values. Keeping it as a 0/1 numeric is a hack. So, create a new
# variable, `vote` which is either Democrat or Republican. In order to make the
# coefficients have the same sign as they did on Tuesday, I change the default
# ordering of the factor levels.

nes <- read_rds("ch13_nes.rds") %>% 
  mutate(vote = factor(ifelse(dvote == 1, "Democrat", "Republican"), 
                          levels = c("Republican", "Democrat")))
```


# Before we start

Here is the [chapter titled "Classification"](https://davidkane9.github.io/PPBDS/13-classification.html) that this class is based on. The data has been taken from the National Election Survey. Note that both ideology and party are measured in 7 point scales. `ideology` ranges from Strong liberal (1) to Strong Conservative (7). `party` ranges from Strong Democrat (1) to Strong Republican (7). `income` is measured on a 5 point scale ranging from very poor (1) to very rich (5). You may treat these variables as continuous. `vote`  is our outcome variable. It is "Democrat" if the person prefers the Democratic candidate for President and "Republican" otherwise.

We are using the [**tidymodels** collection of packages](https://www.tidymodels.org/) today, the more modern/professional approach to creating models in R.

# Scene 1

**Prompt:** Following [the approach](https://davidkane9.github.io/PPBDS/13-classification.html#professional-models) outlined in the *Primer*, use **tidymodels** to estimate a logistic regression in which `vote` is the dependent variable and `ideology`, `income`, `gender`, and `race` are the independent variables.

```{r scene1, echo=FALSE}

logistic_mod <- logistic_reg() %>%
  set_engine("glm")

logistic_fit <- fit(logistic_mod,
                    factor(dvote) ~ (ideology + income + gender + race),
                    data = nes)

logistic_fit$fit %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>% 
  mutate(estimate_divided = estimate / 4) %>% 
  mutate(conf.low_divided = conf.low / 4) %>% 
  mutate(conf.high_divided = conf.high / 4)

```


Useful functions include `logistic_reg()`, `set_engine()`, and `fit()`.

Interpret the coefficient of `income`.

Write a sentence to your smart-but-not-mathematical boss which explains the association of income with preference for the Democratic candidate. Include a notion of uncertainty.

As income increases by a dollar, the probability that a person voted for a Democrat changes by -0.05. We could be off by a bit, but we're 95% sure that the probability changes somewhere between -0.068 and -0.040. 

# Scene 2

**Prompt:** Review [CART models](https://davidkane9.github.io/PPBDS/13-classification.html#classification-and-regression-trees-cart). Using the all the same independent variables as in Scene 1, estimate a CART model in which `vote` is the dependent variable.

```{r}

tree_mod <- decision_tree() %>%  
  set_engine("rpart",model = TRUE) %>%
  set_mode("classification")

vote_tree <- fit(tree_mod,
                    factor(dvote) ~ (ideology + income + gender + race),
                    data = nes)

vote_tree$fit

vote_tree$fit %>% 
  prp(extra = 6, varlen = 0, faclen = 0)

```


Hint: In addition to the functions you used in Scene 1, you may find `set_mode()` and `prp()` to be helpful.

Print the fitted model.

Plot the fitted model. You may find the `prp()` function to be useful. I like `extra = 6` as an argument. Read the help page to understand what it means.

Write two sentences explaining to your smart-but-not-mathematical boss what the model means and how he should interpret it. Make sure to discuss why some variables, like gender, are not used in the model. He thinks that gender matters to voting!

This is a flow chart that uses the most significant factors to determine the probability of whether someone will vote for a Democrat. This tells us that if someone does not have an ideology of 4 or higher, there's a 79% change they voted for a Democrat. If someone has an ideology of 4 or higher and 

Optional: Can you write a simple dplr pipe which does, more or less, what the CART is doing, at least for the first node, and then confirms the numbers in the plot?




# Scene 3

**Prompt:** Following [the approach](https://davidkane9.github.io/PPBDS/13-classification.html#random-forests) outlined in the *Primer*, use **tidymodels** to estimate a random forest model in which `vote` is the dependent variable and `ideology`, `income`, `gender`, and `race` are the independent variables. 

Hint: In addition to the functions you used in Scenes 1 and 2, you may find `rand_forest()` to be helpful.

```{r}

forest_mod <- rand_forest() %>%
  set_engine("randomForest") %>%
  set_mode("classification")

vote_forest <- fit(forest_mod,
                    factor(dvote) ~ (ideology + income + gender + race),
                    data = nes)

vote_forest
```


Print out and interpret the model.

How many trees did the forest use? Would we get better results if we used a different number of trees?

What is OOB error?

# Scene 4

**Prompt:** Weâ€™ve explored three ways to model binary responses in this chapter. Compare the accuracy of predictions of `logistic` vs `cart` vs `forest` on our `nes` data. Here are the steps to follow:

1. Use the models we have already estimated.

2. Add columns of each model's prediction on `nes` by using `mutate` on `nes`. You should use `predict` and feed it the saved model and `new_data = nes`. `pull()` the `.pred_class` from the prediction. 

3. Create a tibble with the accuracy of the prediction columns of each model.

4. Is accuracy the metric to use? Would using sensitivity or specificity be more appropriate?

```{r}

nes %>% 
  mutate(logistic_fit_er = predict(logistic_fit))


house_preds <- nes %>%
  mutate(modal = 1, logistic = predict(logistic_fit, new_data = nes) %>% pull(.pred_class), 
         linear = (predict(logistic_fit, new_data = nes) %>%  pull(.pred_class)), 
         tree = predict(vote_tree, new_data = nes) %>% pull(.pred_class),
         forest = predict(vote_forest, new_data = nes) %>% pull(.pred_class))

test <- tibble(modal = mean(house_preds$modal == house_preds$dvote),
       logistic = mean(house_preds$logistic == house_preds$dvote),
       linear = mean(house_preds$linear == house_preds$dvote),
       tree = mean(house_preds$tree == house_preds$dvote),
       forest = mean(house_preds$forest == house_preds$dvote))
  
```


Note:
Accuracy = (Actual 1 as 1 + Actual 0 as 0) / n
Sensitivity = The proportion of actual 1s classified as 1
Specificity = The proportion of actual 0s classified as 0



# Challenge Problem

Make a cool animation with the nes data, using [this package](https://github.com/daranzolin/d3rain). Put the animation in an Rpubs page. Here are the animations which students in 1006 made yesterday (with different data):

https://rpubs.com/rmckenzie11/599663   
https://rpubs.com/evelyncai/d3rain_practice   
https://rpubs.com/diego_martinez/d3rain   

Surely, you can make something better than what they did! You also have a much richer data set to play with than they had. Maybe separate panels for different years. Maybe voters rain down from different income or ideology categories across the top? There are many possibilities! Add a link with to [today's Preceptor's Notes](https://piazza.com/class/k5y1jx0s5ibe1?cid=721).
